var documenterSearchIndex = {"docs":
[{"location":"slimfly/#Low-diameter-Topology","page":"Slim Fly topology","title":"Low-diameter Topology","text":"","category":"section"},{"location":"slimfly/","page":"Slim Fly topology","title":"Slim Fly topology","text":"Example: Slim Fly, Dragonfly, Xpander, HyperX ...","category":"page"},{"location":"slimfly/#Slim-Fly","page":"Slim Fly topology","title":"Slim Fly","text":"","category":"section"},{"location":"slimfly/","page":"Slim Fly topology","title":"Slim Fly topology","text":"low-diameter topology with diameter 2\nutilizes high-radix routers\nlowest network diameter among the low-diameter topologies (Besta et al. 2020)","category":"page"},{"location":"slimfly/","page":"Slim Fly topology","title":"Slim Fly topology","text":"","category":"page"},{"location":"slimfly/","page":"Slim Fly topology","title":"Slim Fly topology","text":"Optimization problem:","category":"page"},{"location":"slimfly/","page":"Slim Fly topology","title":"Slim Fly topology","text":"objective: maximize the number of endpoints for a given router radix\nconstraints: fixed network diameter and router radix","category":"page"},{"location":"slimfly/","page":"Slim Fly topology","title":"Slim Fly topology","text":"","category":"page"},{"location":"slimfly/#Advantages","page":"Slim Fly topology","title":"Advantages","text":"","category":"section"},{"location":"slimfly/","page":"Slim Fly topology","title":"Slim Fly topology","text":"Using cost-effective fiber cables","category":"page"},{"location":"slimfly/","page":"Slim Fly topology","title":"Slim Fly topology","text":"significantly lower construction and operating costs\nlower latency","category":"page"},{"location":"slimfly/#Problems","page":"Slim Fly topology","title":"Problems","text":"","category":"section"},{"location":"slimfly/","page":"Slim Fly topology","title":"Slim Fly topology","text":"Reference: Multipath Routing for Low-Diameter Network Topologies on InfiniBand Architecture (Nils Blach)","category":"page"},{"location":"slimfly/","page":"Slim Fly topology","title":"Slim Fly topology","text":"low-diameter topologies utilize high-radix switches to achieve lower latency and higher bandwidth","category":"page"},{"location":"slimfly/","page":"Slim Fly topology","title":"Slim Fly topology","text":"Problem 1: Congestion-prone traffic","category":"page"},{"location":"slimfly/","page":"Slim Fly topology","title":"Slim Fly topology","text":"Reason: There is almost always only a single shortest path available between endpoint paris. Thus oversubscribing shortest paths due to inadequate routing (traditional routing schemes)\nApproach: diversity of shortest paths needed","category":"page"},{"location":"slimfly/","page":"Slim Fly topology","title":"Slim Fly topology","text":"=> exploit both minimal and almost-minimal paths","category":"page"},{"location":"slimfly/","page":"Slim Fly topology","title":"Slim Fly topology","text":"=> combines layered routing and InfiniBand's LID masking","category":"page"},{"location":"slimfly/#FatPaths","page":"Slim Fly topology","title":"FatPaths","text":"","category":"section"},{"location":"slimfly/","page":"Slim Fly topology","title":"Slim Fly topology","text":"Reference: FatPaths: Routing in Supercomputers and Data Centers when Shortest Paths Fall Short","category":"page"},{"location":"slimfly/","page":"Slim Fly topology","title":"Slim Fly topology","text":"the first routing architecture targeted the issue regarding the lack of the path diversity for low-diameter topologies deployed on Ethernet","category":"page"},{"location":"benchmarks/#Benchmarks","page":"Benchmark","title":"Benchmarks","text":"","category":"section"},{"location":"benchmarks/#Guideline:-from-openmpi.org-FAQ","page":"Benchmark","title":"Guideline: from openmpi.org FAQ","text":"","category":"section"},{"location":"benchmarks/","page":"Benchmark","title":"Benchmark","text":"Q: I want to run some performance benchmarks with Open MPI.>How do I do that?","category":"page"},{"location":"benchmarks/","page":"Benchmark","title":"Benchmark","text":"Running benchmarks is an extremely difficult task to do correctly. There are many, many factors to take into account; it is not as simple as just compiling and running a stock benchmark application. This FAQ entry is by no means a definitive guide, but it does try to offer some suggestions for generating accurate, meaningful benchmarks.","category":"page"},{"location":"benchmarks/","page":"Benchmark","title":"Benchmark","text":"Decide exactly what you are benchmarking and setup your system accordingly. For example, if you are trying to benchmark maximum performance, then many of the suggestions listed below are extremely relevant (be the only user on the systems and network in question, be the only software running, use processor affinity, etc.). If you're trying to benchmark average performance, some of the suggestions below may be less relevant. Regardless, it is critical to know exactly what you're trying to benchmark, and know (not guess) both your system and the benchmark application itself well enough to understand what the results mean. To be specific, many benchmark applications are not well understood for exactly what they are testing. There have been many cases where users run a given benchmark application and wrongfully conclude that their system's performance is bad — solely on the basis of a single benchmark that they did not understand. Read the documentation of the benchmark carefully, and possibly even look into the code itself to see exactly what it is testing.","category":"page"},{"location":"benchmarks/","page":"Benchmark","title":"Benchmark","text":"Case in point: not all ping-pong benchmarks are created equal. Most users assume that a ping-pong benchmark is a ping-pong benchmark is a ping-pong benchmark. But this is not true; the common ping-pong benchmarks tend to test subtly different things (e.g., NetPIPE, TCP bench, IMB, OSU, etc.). Make sure you understand what your benchmark is actually testing.","category":"page"},{"location":"benchmarks/","page":"Benchmark","title":"Benchmark","text":"Make sure that you are the only user on the systems where you are running the benchmark to eliminate contention from other processes. Make sure that you are the only user on the entire network / interconnect to eliminate network traffic contention from other processes. This is usually somewhat difficult to do, especially in larger, shared systems. But your most accurate, repeatable results will be achieved when you are the only user on the entire network. Disable all services and daemons that are not being used. Even \"harmless\" daemons consume system resources (such as RAM) and cause \"jitter\" by occasionally waking up, consuming CPU cycles, reading or writing to disk, etc. The optimum benchmark system has an absolute minimum number of system services running. Use processor affinity on multi-processor/core machines to disallow the operating system from swapping MPI processes between processors (and causing unnecessary cache thrashing, for example). On NUMA architectures, having the processes getting bumped from one socket to another is more expensive in terms of cache locality (with all of the cache coherency overhead that comes with the lack of it) than in terms of hypertransport routing (see below).","category":"page"},{"location":"benchmarks/","page":"Benchmark","title":"Benchmark","text":"Non-NUMA architectures such as Intel Woodcrest have a flat access time to the South Bridge, but cache locality is still important so CPU affinity is always a good thing to do.","category":"page"},{"location":"benchmarks/","page":"Benchmark","title":"Benchmark","text":"Be sure to understand your system's architecture, particularly with respect to the memory, disk, and network characteristics, and test accordingly. For example, on NUMA architectures, most common being Opteron, the South Bridge is connected through a hypertransport link to one CPU on one socket. Which socket depends on the motherboard, but it should be described in the motherboard documentation (it's not always socket 0!). If a process on the other socket needs to write something to a NIC on a PCIE bus behind the South Bridge, it needs to first hop through the first socket. On modern machines (circa late 2006), this hop cost usually something like 100ns (i.e., 0.1 us). If the socket is further away, like in a 4- or 8-socket configuration, there could potentially be more hops, leading to more latency. Compile your benchmark with the appropriate compiler optimization flags. With some MPI implementations, the compiler wrappers (like mpicc, mpif90, etc.) add optimization flags automatically. Open MPI does not. Add -O or other flags explicitly. Make sure your benchmark runs for a sufficient amount of time. Short-running benchmarks are generally less accurate because they take fewer samples; longer-running jobs tend to take more samples. If your benchmark is trying to benchmark extremely short events (such as the time required for a single ping-pong of messages): Perform some \"warmup\" events first. Many MPI implementations (including Open MPI) — and other subsystems upon which the MPI uses — may use \"lazy\" semantics to setup and maintain streams of communications. Hence, the first event (or first few events) may well take significantly longer than subsequent events. Use a high-resolution timer if possible — gettimeofday() only returns millisecond precision (sometimes on the order of several microseconds). Run the event many, many times (hundreds or thousands, depending on the event and the time it takes). Not only does this provide more samples, it may also be necessary, especially when the precision of the timer you're using may be several orders of magnitude less precise than the event you're trying to benchmark. Decide whether you are reporting minimum, average, or maximum numbers, and have good reasons why. Accurately label and report all results. Reproducibility is a major goal of benchmarking; benchmark results are effectively useless if they are not precisely labeled as to exactly what they are reporting. Keep a log and detailed notes about the exact system configuration that you are benchmarking. Note, for example, all hardware and software characteristics (to include hardware, firmware, and software versions as appropriate).","category":"page"},{"location":"performance/#Performance","page":"Performance","title":"Performance","text":"","category":"section"},{"location":"performance/#Prediction-models","page":"Performance","title":"Prediction models","text":"","category":"section"},{"location":"performance/","page":"Performance","title":"Performance","text":"All of the following models can provide useful insights into various aspects of different algorithms and their relative performances (source: Pjesivac-Grbovic et al.)","category":"page"},{"location":"performance/","page":"Performance","title":"Performance","text":"Hockney, LogP/LogGP, and PLogP","category":"page"},{"location":"performance/#Performance-analysis-of-MPI-collective-operations","page":"Performance","title":"Performance analysis of MPI collective operations","text":"","category":"section"},{"location":"performance/","page":"Performance","title":"Performance","text":"","category":"page"},{"location":"performance/","page":"Performance","title":"Performance","text":"","category":"page"},{"location":"performance/","page":"Performance","title":"Performance","text":"The personal goal is to improve the MPI Reduce algorithm on the cluster. The following section contains the results using the algorithms from the tuned module of Open MPI","category":"page"},{"location":"performance/#osu_reduce","page":"Performance","title":"osu_reduce","text":"","category":"section"},{"location":"performance/","page":"Performance","title":"Performance","text":"Followingly we run the benchmarks on 200 nodes performing 100 iterations for which selected size, and use osu_micro_benchmarks to measure the performance of the system. We choose the algorithms using the -mca switch and specify the wanted algorithms by -mca coll_tuned_use_dynamic_rules 1 -mca coll_tuned_reduce_algorithm [0~6] where the number of the already implemented algorithms varies from 0 to 6","category":"page"},{"location":"performance/","page":"Performance","title":"Performance","text":"# the passed flags of the following command does the following\n# i). makes sure that we are running the program using IB\n# ii). enable dynamic choice of the algorithm to be performed given a specific number\n$(which mpirun) -x PATH=$PATH -x LD_LIBRARY_PATH=$LD_LIBRARY_PATH -mca btl self,openib -mca btl_openib_allow_ib 1 -mca btl_openib_if_include mlx4_0 -mca btl_openib_ib_path_selection_strategy 1 -mca btl_openib_max_lmc 0 -mca btl_openib_enable_apm_over_lmc 0 -mca btl_openib_btls_per_lid 1 -mca pml bfo -mca btl_openib_ib_path_record_service_level 1 -mca coll_tuned_use_dynamic_rules 1 -mca coll_tuned_reduce_algorithm 6 --npernode 1 --hostfile ./200nodes osu_reduce -f -m 100000:5242880 -i 100 -M 50000000","category":"page"},{"location":"performance/","page":"Performance","title":"Performance","text":"The algorithms we can choose are the following","category":"page"},{"location":"performance/","page":"Performance","title":"Performance","text":"/* valid values for coll_tuned_reduce_forced_algorithm */\nstatic mca_base_var_enum_value_t reduce_algorithms[] = {\n    {0, \"ignore\"},\n    {1, \"linear\"},\n    {2, \"chain\"},\n    {3, \"pipeline\"},\n    {4, \"binary\"},\n    {5, \"binomial\"},\n    {6, \"in-order_binary\"},\n    {0, NULL}\n};","category":"page"},{"location":"performance/","page":"Performance","title":"Performance","text":"Algorithm 0: ignore","category":"page"},{"location":"performance/","page":"Performance","title":"Performance","text":"# OSU MPI Reduce Latency Test v6.2\n# Size       Avg Latency(us)   Min Latency(us)   Max Latency(us)  Iterations\n100000                132.14             31.23            831.04         100\n200000                322.67             62.81           1563.97         100\n400000               2834.61            146.85           3965.61         100\n800000               8297.08            324.86           9236.95         100\n1600000             35948.33            736.15          37948.87         100\n3200000            276274.93           1630.58         281278.16         100","category":"page"},{"location":"performance/","page":"Performance","title":"Performance","text":"Algorithm 1: linear","category":"page"},{"location":"performance/","page":"Performance","title":"Performance","text":"# OSU MPI Reduce Latency Test v6.2\n# Size       Avg Latency(us)   Min Latency(us)   Max Latency(us)  Iterations\n100000               6487.39            443.43          12355.04         100\n200000               9448.29            552.40          18120.21         100\n400000              16387.96            600.33          31946.03         100\n800000              30068.94            666.30          59422.85         100\n1600000             58182.33            946.41         115545.30         100\n3200000            113780.66           1140.69         227546.26         100","category":"page"},{"location":"performance/","page":"Performance","title":"Performance","text":"Algorithm 2: chain","category":"page"},{"location":"performance/","page":"Performance","title":"Performance","text":"# OSU MPI Reduce Latency Test v6.2\n# Size       Avg Latency(us)   Min Latency(us)   Max Latency(us)  Iterations\n100000               1666.99             42.11           3464.73         100\n200000               5448.95            177.72          11015.41         100\n400000               8196.34            260.80          16709.97         100\n800000              13808.33            420.40          28388.52         100\n1600000             25167.34            703.10          51974.58         100\n3200000             45377.38           1069.22          93009.15         100","category":"page"},{"location":"performance/","page":"Performance","title":"Performance","text":"Algorithm 3: pipeline","category":"page"},{"location":"performance/","page":"Performance","title":"Performance","text":"# OSU MPI Reduce Latency Test v6.2\n# Size       Avg Latency(us)   Min Latency(us)   Max Latency(us)  Iterations\n100000               6712.54             57.09          13386.94         100\n200000              21579.71            198.61          42843.28         100\n400000              32205.14            291.74          64115.23         100\n800000              54162.76            485.81         107843.99         100\n1600000             99080.83            879.45         197513.47         100\n3200000            178693.16           1389.62         355750.96         100","category":"page"},{"location":"performance/","page":"Performance","title":"Performance","text":"Algorithm 4: binary","category":"page"},{"location":"performance/","page":"Performance","title":"Performance","text":"# OSU MPI Reduce Latency Test v6.2\n# Size       Avg Latency(us)   Min Latency(us)   Max Latency(us)  Iterations\n100000                229.67             41.72            879.07         100\n200000                706.75            178.87           2515.09         100\n400000               1144.38            257.11           4142.77         100\n800000               1973.52            406.54           6808.06         100\n1600000              3666.60            718.00          12641.29         100\n3200000              6484.51           1079.66          23392.05         100","category":"page"},{"location":"performance/","page":"Performance","title":"Performance","text":"Algorithm 5: binomial","category":"page"},{"location":"performance/","page":"Performance","title":"Performance","text":"# OSU MPI Reduce Latency Test v6.2\n# Size       Avg Latency(us)   Min Latency(us)   Max Latency(us)  Iterations\n100000                122.61             39.89            609.98         100\n200000                396.09            160.07           1198.21         100\n400000                584.20            212.95           1904.71         100\n800000               1006.74            332.68           3487.79         100\n1600000              1840.96            551.60           6741.70         100\n3200000              3161.08            946.51          12395.23         100","category":"page"},{"location":"performance/","page":"Performance","title":"Performance","text":"Algorithm 6: in-order_binary","category":"page"},{"location":"performance/","page":"Performance","title":"Performance","text":"# OSU MPI Reduce Latency Test v6.2\n# Size       Avg Latency(us)   Min Latency(us)   Max Latency(us)  Iterations\n100000                176.08             40.90            914.90         100\n200000                790.12            187.62           3052.36         100\n400000                911.30            247.37           3559.09         100\n800000               1510.80            390.38           5900.00         100\n1600000              2820.92            701.73          11307.00         100\n3200000              5039.58           1072.93          20802.54         100","category":"page"},{"location":"performance/#osu_allreduce","page":"Performance","title":"osu_allreduce","text":"","category":"section"},{"location":"performance/","page":"Performance","title":"Performance","text":"We launched the osu-benchmarks using the following argument, choosing the desired allreduce algorithms $(which mpirun) -x PATH=$PATH -x LD_LIBRARY_PATH=$LD_LIBRARY_PATH -mca btl self,openib -mca btl_openib_allow_ib 1 -mca btl_openib_if_include mlx4_0 -mca btl_openib_ib_path_selection_strategy 1 -mca btl_openib_max_lmc 0 -mca btl_openib_enable_apm_over_lmc 0 -mca btl_openib_btls_per_lid 1 -mca pml bfo -mca btl_openib_ib_path_record_service_level 1 -mca coll_tuned_use_dynamic_rules 1 -mca coll_tuned_allreduce_algorithm 1 --npernode 1 --hostfile ./200nodes osu_allreduce -f -m 100000:5242880 -i 100 -M 50000000","category":"page"},{"location":"performance/","page":"Performance","title":"Performance","text":"# avaliable existing algorithms\nstatic mca_base_var_enum_value_t allreduce_algorithms[] = {\n    {0, \"ignore\"},\n    {1, \"basic_linear\"},\n    {2, \"nonoverlapping\"},\n    {3, \"recursive_doubling\"},\n    {4, \"ring\"},\n    {5, \"segmented_ring\"},\n    {0, NULL}\n};","category":"page"},{"location":"performance/","page":"Performance","title":"Performance","text":"Algorithm 0: ignore","category":"page"},{"location":"performance/","page":"Performance","title":"Performance","text":"# OSU MPI Allreduce Latency Test v6.2\n# Size       Avg Latency(us)   Min Latency(us)   Max Latency(us)  Iterations\n100000               1181.07           1168.71           1188.92         100\n200000               1462.62           1442.96           1479.07         100\n400000               1818.38           1803.78           1834.10         100\n800000               3303.89           3186.16           3449.73         100\n1600000              3171.33           3154.79           3184.00         100\n3200000              9006.89           8725.43           9220.24         100","category":"page"},{"location":"performance/","page":"Performance","title":"Performance","text":"Algorithm 1: basic_linear","category":"page"},{"location":"performance/","page":"Performance","title":"Performance","text":"# OSU MPI Allreduce Latency Test v6.2\n# Size       Avg Latency(us)   Min Latency(us)   Max Latency(us)  Iterations\n100000              16879.04          16450.94          17293.41         100\n200000              26054.80          25808.69          26254.45         100\n400000              43064.00          39144.78          45117.78         100\n800000              82261.62          70034.41          84763.74         100\n1600000            162705.45         148548.90         165350.29         100\n3200000            323221.61         308570.94         326295.46         100","category":"page"},{"location":"performance/","page":"Performance","title":"Performance","text":"Algorithm 2: nonoverlapping","category":"page"},{"location":"performance/","page":"Performance","title":"Performance","text":"# OSU MPI Allreduce Latency Test v6.2\n# Size       Avg Latency(us)   Min Latency(us)   Max Latency(us)  Iterations\n100000               1163.39            938.96           1247.31         100\n200000               1752.41           1531.61           1793.60         100\n400000               4019.97           3017.85           4804.85         100\n800000              11037.44           9612.77          11960.02         100\n1600000             39830.66          37595.83          40870.52         100\n3200000            325071.04         318648.53         327393.94         100","category":"page"},{"location":"performance/","page":"Performance","title":"Performance","text":"Algorithm 3: recursive_doubling","category":"page"},{"location":"performance/","page":"Performance","title":"Performance","text":"# OSU MPI Allreduce Latency Test v6.2\n# Size       Avg Latency(us)   Min Latency(us)   Max Latency(us)  Iterations\n100000                848.37            774.12            914.16         100\n200000               1581.94           1467.31           1702.09         100\n400000               2763.76           2534.04           3069.25         100\n800000               5172.03           4515.14           5691.36         100\n1600000              9693.43           8079.93          10726.02         100\n3200000             18994.18          15796.30          20934.79         100","category":"page"},{"location":"performance/","page":"Performance","title":"Performance","text":"Algorithm 4: ring","category":"page"},{"location":"performance/","page":"Performance","title":"Performance","text":"# OSU MPI Allreduce Latency Test v6.2\n# Size       Avg Latency(us)   Min Latency(us)   Max Latency(us)  Iterations\n100000                851.32            779.69            908.17         100\n200000               3113.09           2909.20           3333.70         100\n400000               2611.02           2342.51           2907.94         100\n800000               5028.45           4326.89           5594.54         100\n1600000              9659.68           7959.01          10758.98         100\n3200000             19051.46          15704.57          20881.06         100","category":"page"},{"location":"performance/","page":"Performance","title":"Performance","text":"Algorithm 5: segmented_ring","category":"page"},{"location":"performance/","page":"Performance","title":"Performance","text":"# OSU MPI Allreduce Latency Test v6.2\n# Size       Avg Latency(us)   Min Latency(us)   Max Latency(us)  Iterations\n100000               1174.00           1165.55           1182.63         100\n200000               1452.26           1434.70           1476.29         100\n400000               1782.82           1770.29           1792.72         100\n800000               3295.72           3154.78           3411.68         100\n1600000              3181.24           3150.86           3202.50         100\n3200000              8853.35           8709.90           9026.74         100","category":"page"},{"location":"performance/","page":"Performance","title":"Performance","text":"osu_get_latency","category":"page"},{"location":"performance/","page":"Performance","title":"Performance","text":"","category":"page"},{"location":"performance/","page":"Performance","title":"Performance","text":"osu_latency","category":"page"},{"location":"performance/","page":"Performance","title":"Performance","text":"","category":"page"},{"location":"performance/","page":"Performance","title":"Performance","text":"osu_latency_mp","category":"page"},{"location":"performance/","page":"Performance","title":"Performance","text":"","category":"page"},{"location":"performance/","page":"Performance","title":"Performance","text":"osu_latency_mt","category":"page"},{"location":"performance/","page":"Performance","title":"Performance","text":"","category":"page"},{"location":"performance/","page":"Performance","title":"Performance","text":"osu_alltoall","category":"page"},{"location":"performance/","page":"Performance","title":"Performance","text":"","category":"page"},{"location":"performance/","page":"Performance","title":"Performance","text":"osu_alltoallv","category":"page"},{"location":"performance/","page":"Performance","title":"Performance","text":"\n","category":"page"},{"location":"performance/","page":"Performance","title":"Performance","text":"osu_alltoallw","category":"page"},{"location":"performance/","page":"Performance","title":"Performance","text":"\n","category":"page"},{"location":"readings/#Readings","page":"Readings","title":"Readings","text":"","category":"section"},{"location":"readings/#Bachelor-Thesis:-Multipath-Routing-for-Low-Diameter-Network-Topology-on-InfiniBand-Architecture-Nils-Blach","page":"Readings","title":"Bachelor Thesis: Multipath Routing for Low-Diameter Network Topology on InfiniBand Architecture - Nils Blach","text":"","category":"section"},{"location":"trouble_shooting/#Trouble-Shooting","page":"Trouble shooting","title":"Trouble Shooting","text":"","category":"section"},{"location":"trouble_shooting/#Maybe-Useful-Q-and-A-from-openmpi.org","page":"Trouble shooting","title":"Maybe-Useful Q&A from openmpi.org","text":"","category":"section"},{"location":"trouble_shooting/","page":"Trouble shooting","title":"Trouble shooting","text":"prefix","category":"page"},{"location":"trouble_shooting/","page":"Trouble shooting","title":"Trouble shooting","text":"/share/openmpi/mca-btl-openib-hca-params.ini","category":"page"},{"location":"algorithm/#Algorithm","page":"Algorithm","title":"Algorithm","text":"","category":"section"},{"location":"algorithm/","page":"Algorithm","title":"Algorithm","text":"The personal goal is to improve the MPI Reduce algorithm on the cluster.","category":"page"},{"location":"algorithm/#MPI-Reduce","page":"Algorithm","title":"MPI Reduce","text":"","category":"section"},{"location":"algorithm/#MPI-Scatter/Gather","page":"Algorithm","title":"MPI Scatter/Gather","text":"","category":"section"},{"location":"algorithm/#My-debugging-section","page":"Algorithm","title":"My debugging section","text":"","category":"section"},{"location":"algorithm/","page":"Algorithm","title":"Algorithm","text":"Following code snippet is based on the implementation that Marcel has implemented and an unoptimized one, for personal use to compare","category":"page"},{"location":"algorithm/","page":"Algorithm","title":"Algorithm","text":"[dphpc@slimflysmw tuned]$ diff -y --suppress-common-lines coll_tuned_alltoall.c /home/dphpc/scratch/dphpc/wyou/openmpi/ompi/mca/coll/tuned/coll_tuned_alltoall.c \n/*                                                            <\narameter \"coll_tuned_alltoall_algorithm\" (current value: \"ign <\nWhich alltoall algorithm is used. Can be locked down to choic <\nValid values:                                                 <\n0:\"ignore\",                                                   <\n1:\"linear\",                                                   <\n2:\"pairwise\",                                                 <\n3:\"modified_bruck\",                                           <\n4:\"linear_sync\",                                              <\n5:\"two_proc\"                                                  <\n*/                                                            <\n                                                              <\n#include<stdio.h>                                             <\n#include<stdlib.h>                                            <\n                                                              <\nvoid FisherYatesAllToAll(int * arr, unsigned int size, unsign <\n    srand(seed);                                              <\n    for(int i = size - 1; i > 0; --i){                        <\n        int j = rand() % i;                                   <\n        int tmp = arr[i];                                     <\n        arr[i] = arr[j];                                      <\n        arr[j] = tmp;                                         <\n    }                                                         <\n}                                                             <\n                                                              <\nint str2int(const char * s){                                  <\n   char *ptr;                                                 <\n   long ret;                                                  <\n                                                              <\n   ret = strtol(s, &ptr, 10);                                 <\n   return (int) ret;                                          <\n}                                                             <\n                                                              <\n                                                              <\n                                                              <\n                                                              <\n    //printf(\"Ayo mandem using da custom inplace algorithm ye <\n                                                              <\n                                                              <\n    int* idx = (int*) malloc(sizeof(unsigned int) * size);    <\n                                                              <\n    for(int j = 0; j < size; ++j){                            <\n        idx[j] = j;                                           <\n    }                                                         <\n                                                              <\n    FisherYatesAllToAll(idx, size, size);                     <\n                                                              <\n            if (idx[i] == rank) {                             |             if (i == rank) {\n                                                       (char  |                                                        (char \n                err = MCA_PML_CALL(recv ((char *) rbuf + max_ |                 err = MCA_PML_CALL(irecv ((char *) rbuf + max\n                                          idx[j], MCA_COLL_BA |                                           j, MCA_COLL_BASE_TA\n                err = MCA_PML_CALL(send ((char *) tmp_buffer, |                 err = MCA_PML_CALL(isend ((char *) tmp_buffer\n                                          idx[j], MCA_COLL_BA |                                           j, MCA_COLL_BASE_TA\n                                          comm));//, preq++)) |                                           comm, preq++));\n            } else if (idx[j] == rank) {                      |             } else if (j == rank) {\n                                                       (char  |                                                        (char \n                err = MCA_PML_CALL(recv ((char *) rbuf + max_ |                 err = MCA_PML_CALL(irecv ((char *) rbuf + max\n                                          idx[i], MCA_COLL_BA |                                           i, MCA_COLL_BASE_TA\n                err = MCA_PML_CALL(send ((char *) tmp_buffer, |                 err = MCA_PML_CALL(isend ((char *) tmp_buffer\n                                          idx[i], MCA_COLL_BA |                                           i, MCA_COLL_BASE_TA\n                                          comm));//, preq++)) |                                           comm, preq++));\n           // err = ompi_request_wait_all (2, tuned_module->t |             err = ompi_request_wait_all (2, tuned_module->tun\n            //if (MPI_SUCCESS != err) { goto error_hndl; }    |             if (MPI_SUCCESS != err) { goto error_hndl; }\n            //mca_coll_tuned_free_reqs(tuned_module->tuned_da |             mca_coll_tuned_free_reqs(tuned_module->tuned_data\n                                                              <\n                                                              <\n    free(idx);                                                <\n                                                              <\n    char * env;                                               <\n    int npernode = 0;                                         <\n    int nperswitch = 0;                                       <\n    int debug = 0;                                            <\n    int mode = 0;                                             <\n                                                              <\n    env = getenv(\"SF_NPERNODE\");                              <\n    if(env){npernode = str2int(env);}                         <\n                                                              <\n    env = getenv(\"SF_NPERSWITCH\");                            <\n    if(env){nperswitch = str2int(env);}                       <\n                                                              <\n    env = getenv(\"SF_DEBUG\");                                 <\n    if(env){debug = str2int(env);}                            <\n                                                              <\n    env = getenv(\"SF_ALLTOALL_MODE\");                         <\n    if(env){mode = str2int(env);}                             <\n                                                              <\n    if(mode == 0){                                            <\n        goto base;                                            <\n    } else if (mode == 1){                                    <\n        goto random_scheduling;                               <\n    } else if (mode == 2) {                                   <\n        goto custom_scheduling;                               <\n    } else if (mode == 3){                                    <\n        goto random_grid;                                     <\n    } else {                                                  <\n        goto err_hndl;                                        <\n    }                                                         <\n                                                              <\n    if(debug){                                                <\n        printf(\"RANK: %i\\nnpernode=%i\\nnperswitch=%i\\nmode=%i <\n    }                                                         <\n                                                              <\n    // DEFAULT                                                <\n  base:                                                       <\n    if(debug == 3){                                           <\n        printf(\"Using default scheduling.\\n\");                <\n    }                                                         <\n                                                              <\n        if(debug){                                            <\n            printf(\"Rank %i: sending to %i, receiving from %i <\n        }                                                     <\n                                                              <\n                                                              <\n                                                              |  \n    // RANDOM SCHEDULING                                      <\n random_scheduling:                                           <\n                                                              <\n    if(debug == 3){                                           <\n        printf(\"Using random scheduling.\\n\");                 <\n    }                                                         <\n                                                              <\n    int* idx = (int*) malloc(sizeof(int) * size);             <\n                                                              <\n    for(int j = 0; j < size; ++j){                            <\n        idx[j] = j+1;                                         <\n    }                                                         <\n                                                              <\n    FisherYatesAllToAll(idx, size-1, size);                   <\n                                                              <\n    /* Perform pairwise exchange - starting from 1 so the loc <\n    for (int i = 0; i < size; ++i) {                          <\n        step = idx[i];                                        <\n        /* Determine sender and receiver for this step. */    <\n                                                              <\n        sendto  = (rank + step) % size;                       <\n        recvfrom = (rank + size - step) % size;               <\n                                                              <\n        if(debug){                                            <\n            printf(\"Rank %i: sending to %i, receiving from %i <\n        }                                                     <\n                                                              <\n        /* Determine sending and receiving locations */       <\n        tmpsend = (char*)sbuf + (ptrdiff_t)sendto * sext * (p <\n        tmprecv = (char*)rbuf + (ptrdiff_t)recvfrom * rext *  <\n                                                              <\n        /* send and receive */                                <\n        err = ompi_coll_tuned_sendrecv( tmpsend, scount, sdty <\n                                        MCA_COLL_BASE_TAG_ALL <\n                                        tmprecv, rcount, rdty <\n                                        MCA_COLL_BASE_TAG_ALL <\n                                        comm, MPI_STATUS_IGNO <\n        if (err != MPI_SUCCESS) { line = __LINE__; goto err_h <\n    }                                                         <\n    free(idx);                                                <\n    return MPI_SUCCESS;                                       <\n                                                              <\n    // CUSTOM SCHEDULING                                      <\n custom_scheduling:                                           <\n                                                              <\n    if(debug == 3){                                           <\n        printf(\"Using custom scheduling.\\n\");                 <\n    }                                                         <\n                                                              <\n    int switches = size / nperswitch;                         <\n                                                              <\n                                                              <\n    /* Perform pairwise exchange - starting from 1 so the loc <\n    for (int i = 0; i < nperswitch; ++i){                     <\n        for (int j = 0; j < switches; ++j) {                  <\n            step = nperswitch*j + i;                          <\n                                                              <\n            if(step == 0 || step > size){continue;}           <\n                                                              <\n            /* Determine sender and receiver for this step. * <\n            sendto  = (rank + step) % size;                   <\n            recvfrom = (rank + size - step) % size;           <\n                                                              <\n            if(debug){                                        <\n                printf(\"Rank %i: sending to %i, receiving fro <\n            }                                                 <\n                                                              <\n                                                              <\n            /* Determine sending and receiving locations */   <\n            tmpsend = (char*)sbuf + (ptrdiff_t)sendto * sext  <\n            tmprecv = (char*)rbuf + (ptrdiff_t)recvfrom * rex <\n                                                              <\n            /* send and receive */                            <\n            err = ompi_coll_tuned_sendrecv( tmpsend, scount,  <\n                                            MCA_COLL_BASE_TAG <\n                                            tmprecv, rcount,  <\n                                            MCA_COLL_BASE_TAG <\n                                            comm, MPI_STATUS_ <\n            if (err != MPI_SUCCESS) { line = __LINE__; goto e <\n        }                                                     <\n    }                                                         <\n                                                              <\n    // Do last local copy                                     <\n    /* Determine sender and receiver for this step. */        <\n    sendto  = (rank) % size;                                  <\n    recvfrom = (rank + size) % size;                          <\n                                                              <\n    /* Determine sending and receiving locations */           <\n    tmpsend = (char*)sbuf + (ptrdiff_t)sendto * sext * (ptrdi <\n    tmprecv = (char*)rbuf + (ptrdiff_t)recvfrom * rext * (ptr <\n                                                              <\n    /* send and receive */                                    <\n    err = ompi_coll_tuned_sendrecv( tmpsend, scount, sdtype,  <\n                                    MCA_COLL_BASE_TAG_ALLTOAL <\n                                    tmprecv, rcount, rdtype,  <\n                                    MCA_COLL_BASE_TAG_ALLTOAL <\n                                    comm, MPI_STATUS_IGNORE,  <\n    if (err != MPI_SUCCESS) { line = __LINE__; goto err_hndl; <\n                                                              <\n    return MPI_SUCCESS;                                       <\n random_grid:                                                 <\n    if(debug == 3){                                           <\n        printf(\"Using random grid scheduling.\\n\");            <\n    }                                                         <\n                                                              <\n    // Allocate grid                                          <\n    idx = (int*) malloc(sizeof(int) * size * size);           <\n                                                              <\n    for(int i = 0; i < size; ++i){                            <\n        for(int j = 0; j < size; ++j){                        <\n            if(j < i){                                        <\n                idx[i*size + j] = -1;                         <\n            } else {                                          <\n                idx[i*size + j] = j;                          <\n            }                                                 <\n        }                                                     <\n                                                              <\n        FisherYatesAllToAll(idx + i*size, size, size + i);    <\n    }                                                         <\n                                                              <\n                                                              <\n                                                              <\n    /* Perform pairwise exchange - starting from 1 so the loc <\n                                                              <\n    for (int j = 0; j < size; ++j) {                          <\n        for (int i = 0; i < size; ++i) {                      <\n                                                              <\n            /* Determine sender and receiver for this step. * <\n            // sendto  = (rank + step) % size;                <\n            // recvfrom = (rank + size - step) % size;        <\n            if(idx[i*size + j]==-1){continue;}                <\n                                                              <\n            if(rank != i && rank != idx[i*size + j]){continue <\n            else if(rank == i){                               <\n                sendto = idx[i*size + j];                     <\n                recvfrom = idx[i*size + j];                   <\n            } else {                                          <\n                sendto = i;                                   <\n                recvfrom = i;                                 <\n            }                                                 <\n                                                              <\n                                                              <\n            if(debug){                                        <\n                printf(\"Rank %i: sending to %i, receiving fro <\n            }                                                 <\n                                                              <\n                                                              <\n            /* Determine sending and receiving locations */   <\n            tmpsend = (char*)sbuf + (ptrdiff_t)sendto * sext  <\n            tmprecv = (char*)rbuf + (ptrdiff_t)recvfrom * rex <\n                                                              <\n            /* send and receive */                            <\n            err = ompi_coll_tuned_sendrecv( tmpsend, scount,  <\n                                            MCA_COLL_BASE_TAG <\n                                            tmprecv, rcount,  <\n                                            MCA_COLL_BASE_TAG <\n                                            comm, MPI_STATUS_ <\n            if (err != MPI_SUCCESS) { line = __LINE__; goto e <\n                                                              <\n        }                                                     <\n    }                                                         <\n                                                              <\n    free(idx);                                                <\n    return MPI_SUCCESS;                                       <\n                                                              <\n    char * env;                                               <\n    int debug = 0;                                            <\n                                                              <\n    env = getenv(\"SF_DEBUG\");                                 <\n    if(env){debug = str2int(env);}                            <\n                                                              <\n    if(debug){                                                <\n        printf(\"Using custom Brucks alltoall!\\n\");            <\n    }                                                         <\n                                                              <\n                                                              <\n        if(debug){                                            <\n            printf(\"Delegating to pasic inplace!\\n\");         <\n        }                                                     <\n                                                              <\n    if(debug){                                                <\n        printf(\"Running modified bruck!\\n\");                  <\n    }                                                         <\n                                                              <\n    // Randomize                                              <\n    int* idx = (int*) malloc(sizeof(unsigned int) * size);    <\n                                                              <\n    int len = 0;                                              <\n    for(int j = 1; j < size; j<<=1, ++len){                   <\n        idx[len] = j;                                         <\n    }                                                         <\n                                                              <\n    FisherYatesAllToAll(idx, len, size);                      <\n                                                              <\n                                                              <\n    for (int d = 0; d < len; ++d) {                           |     for (distance = 1; distance < size; distance<<=1) {\n        distance = idx[d];                                    |\n\n\n","category":"page"},{"location":"infiniband_architecture/#InfiniBand-Architecture","page":"InfiniBand Architecture","title":"InfiniBand Architecture","text":"","category":"section"},{"location":"infiniband_architecture/","page":"InfiniBand Architecture","title":"InfiniBand Architecture","text":"based on Virtual Cut Through (VCT)","category":"page"},{"location":"infiniband_architecture/","page":"InfiniBand Architecture","title":"InfiniBand Architecture","text":"RDMA over InfiniBand: Package from one server at user application layer does not need to be copied into the kernel layer and the hardware layer but can be directed transported from the buffer of server 1 to HCAs and then to the buffer of server 2","category":"page"},{"location":"infiniband_architecture/#Routing-on-Slim-Fly-at-CSCS","page":"InfiniBand Architecture","title":"Routing on Slim Fly at CSCS","text":"","category":"section"},{"location":"infiniband_architecture/","page":"InfiniBand Architecture","title":"InfiniBand Architecture","text":"Matter of interest: Intra-subnet routing","category":"page"},{"location":"infiniband_architecture/","page":"InfiniBand Architecture","title":"InfiniBand Architecture","text":"Hardware components:","category":"page"},{"location":"infiniband_architecture/","page":"InfiniBand Architecture","title":"InfiniBand Architecture","text":"Switch: A device that moves packets from one link to another of the same IB subnet (layer2)","category":"page"},{"location":"infiniband_architecture/","page":"InfiniBand Architecture","title":"InfiniBand Architecture","text":"Host Channel Adapter (HCA): device that terminates an IB link and executes transport-level functions and support the verbs interface","category":"page"},{"location":"infiniband_architecture/","page":"InfiniBand Architecture","title":"InfiniBand Architecture","text":"IB Router: A device that transports packets between different IBA subnets. (inter-subnet routing)","category":"page"},{"location":"infiniband_architecture/","page":"InfiniBand Architecture","title":"InfiniBand Architecture","text":"Bridge/Gateway: IB to Ethernet","category":"page"},{"location":"infiniband_architecture/","page":"InfiniBand Architecture","title":"InfiniBand Architecture","text":"","category":"page"},{"location":"infiniband_architecture/","page":"InfiniBand Architecture","title":"InfiniBand Architecture","text":"Other Concepts","category":"page"},{"location":"infiniband_architecture/","page":"InfiniBand Architecture","title":"InfiniBand Architecture","text":"Virtual Lanes (VLs): Each virtual lane uses different buffers to send its packet towards the other side. VL-15 moves traffic of subnet manager specifically; VL-0-7 to move traffic, by default all data goes through VL-0. In order to provide different lane of service based on different usage, we need to map applications to specific virtual lanes.","category":"page"},{"location":"infiniband_architecture/","page":"InfiniBand Architecture","title":"InfiniBand Architecture","text":"Linear Forwarding Table (LFT): is indexed using the destination LID (DLID) of the packet. Helps to determine the output port of a packet at a switch.\nService Level (SL): 4-bit meta-data contained in each packet's header, used to determine the outgoing VL by a SL-to-VL table => thus packets can change virtual lanes at each hop, hardware with different number of virtual lanes can be used together.","category":"page"},{"location":"infiniband_architecture/","page":"InfiniBand Architecture","title":"InfiniBand Architecture","text":"Subnet Manager: most important entity in the network, does all configurations using parameters. Every subnet must have at least one.","category":"page"},{"location":"infiniband_architecture/","page":"InfiniBand Architecture","title":"InfiniBand Architecture","text":"[dphpc@slimflysmw benchmarks]$ sminfo\nibwarn: [55188] mad_rpc_open_port: can't open UMAD port ((null):0)\nsminfo: iberror: failed: Failed to open '(null)' port '0'\n[dphpc@slimflysmw benchmarks]$ saquery -s\nibwarn: [55250] sa_get_handle: umad_open_port on port (null):0 failed\nibpanic: [55250] main: Failed to bind to the SA: Permission denied","category":"page"},{"location":"infiniband_architecture/","page":"InfiniBand Architecture","title":"InfiniBand Architecture","text":"\"within a subnet the switches are of layer-2-switching\"?","category":"page"},{"location":"infiniband_architecture/","page":"InfiniBand Architecture","title":"InfiniBand Architecture","text":"Addressing:","category":"page"},{"location":"infiniband_architecture/","page":"InfiniBand Architecture","title":"InfiniBand Architecture","text":"i). Physical Adress: Global Unique Identifier (GUID)","category":"page"},{"location":"infiniband_architecture/","page":"InfiniBand Architecture","title":"InfiniBand Architecture","text":"64 bits adress\npersistent through reboots => no changes when rebooting\na switch has only one GUID (Port: identify by GUID of the node and the port number)","category":"page"},{"location":"infiniband_architecture/","page":"InfiniBand Architecture","title":"InfiniBand Architecture","text":"ii). L2 Switching Addressing Local Identifier (LID)","category":"page"},{"location":"infiniband_architecture/","page":"InfiniBand Architecture","title":"InfiniBand Architecture","text":"16 bits adress\nused when moving traffic from one node to another node within a subnet\nassigned by the Subnet Manager when port becomes active\nNot persistent through reboots\nUse the LID Mask Control (LMC) value","category":"page"},{"location":"infiniband_architecture/","page":"InfiniBand Architecture","title":"InfiniBand Architecture","text":"LMC=x Rightarrow textNoLIDs = 2^x","category":"page"},{"location":"infiniband_architecture/","page":"InfiniBand Architecture","title":"InfiniBand Architecture","text":"iii). Global Identifier (GID)","category":"page"},{"location":"infiniband_architecture/","page":"InfiniBand Architecture","title":"InfiniBand Architecture","text":"layer 3 address\n128 bits, a combination of the port GUID and the subnet prefix","category":"page"},{"location":"infiniband_architecture/","page":"InfiniBand Architecture","title":"InfiniBand Architecture","text":"Implementation: ","category":"page"},{"location":"infiniband_architecture/","page":"InfiniBand Architecture","title":"InfiniBand Architecture","text":"All comm unication up to and include the transport layer is specified and implemented within the switches and Host >Channel Adapters (HCAs)","category":"page"},{"location":"infiniband_architecture/#Ethernet","page":"InfiniBand Architecture","title":"Ethernet","text":"","category":"section"},{"location":"infiniband_architecture/","page":"InfiniBand Architecture","title":"InfiniBand Architecture","text":"Moving data between the InfiniBand and the Ethernet environment needs the hardware component \"Gateway\"","category":"page"},{"location":"mpitutorial/#Open-MPI","page":"MPI Tutorial","title":"Open MPI","text":"","category":"section"},{"location":"mpitutorial/#Basics","page":"MPI Tutorial","title":"Basics","text":"","category":"section"},{"location":"mpitutorial/","page":"MPI Tutorial","title":"MPI Tutorial","text":"The Open MPI is comprised of three main functional areas:","category":"page"},{"location":"mpitutorial/","page":"MPI Tutorial","title":"MPI Tutorial","text":"i). Modular Component Architecture (MCA) The center of the Open MPI design, acts as the backbone component architecture that provides management to all other layers. Provide services such as accepting run-time parameters from higher-level abstractions like mpirun and pass parameters through the component framework to each individual module.","category":"page"},{"location":"mpitutorial/","page":"MPI Tutorial","title":"MPI Tutorial","text":"ii). Component Frameworks Back-end component framework used to manage modules of each main functional area of Open MPI. Each framework with different policy.","category":"page"},{"location":"mpitutorial/","page":"MPI Tutorial","title":"MPI Tutorial","text":"iii). Modules Self-contained software components, each dedicated to a single job and is composable with other modules in run-time.","category":"page"},{"location":"mpitutorial/","page":"MPI Tutorial","title":"MPI Tutorial","text":"Component programming provides library developer a fine-grained, run-time, user-controlled component selection mechanism. Convinient for our development.","category":"page"},{"location":"mpitutorial/#Setting-MCA-parameters","page":"MPI Tutorial","title":"Setting MCA parameters","text":"","category":"section"},{"location":"mpitutorial/","page":"MPI Tutorial","title":"MPI Tutorial","text":"Syntax:  -mca <key> <value1, ..., valueN>, where the <key> specifies the MCA module to receive the <value>. This is a shortcut to set environment variables of the form OMPI_MCA_<key>=<value> and it overrides the parameters set in mca-params.conf file.","category":"page"},{"location":"mpitutorial/","page":"MPI Tutorial","title":"MPI Tutorial","text":"It is more advisable to pass -mca flag instead of --mca, even though the current openmpi version we are using recognizes it as a valid synonym. It is generally recommended to use the \"-mca\" option, rather than \"–mca\", when running \"mpirun\" with Open MPI. This is because the \"-mca\" option is more widely recognized and documented, and is the more commonly used syntax for specifying MCA parameters. Using \"–mca\" may cause confusion for users who are not familiar with this alternative syntax, or who are using a version of Open MPI that does not support the \"–mca\" option.","category":"page"},{"location":"mpitutorial/","page":"MPI Tutorial","title":"MPI Tutorial","text":"The order in which the values are passed indicate the priority of them respectively.","category":"page"},{"location":"mpitutorial/#Components-for-MPI-collective-communication","page":"MPI Tutorial","title":"Components for MPI collective communication","text":"","category":"section"},{"location":"mpitutorial/#basic","page":"MPI Tutorial","title":"basic","text":"","category":"section"},{"location":"mpitutorial/","page":"MPI Tutorial","title":"MPI Tutorial","text":"basic: the default component used when not using either shared memory or self, contains at least one set of implementations per collective operation.","category":"page"},{"location":"mpitutorial/#sm","page":"MPI Tutorial","title":"sm","text":"","category":"section"},{"location":"mpitutorial/","page":"MPI Tutorial","title":"MPI Tutorial","text":"sm: collectives for use when Open MPI is running completely on a shared memory system","category":"page"},{"location":"mpitutorial/#self","page":"MPI Tutorial","title":"self","text":"","category":"section"},{"location":"mpitutorial/","page":"MPI Tutorial","title":"MPI Tutorial","text":"self: a special feature within MPI for use on the MPI_COMM_SELF communicator","category":"page"},{"location":"mpitutorial/#tuned","page":"MPI Tutorial","title":"tuned","text":"","category":"section"},{"location":"mpitutorial/","page":"MPI Tutorial","title":"MPI Tutorial","text":"tuned is an open MPI collective communications component","category":"page"},{"location":"mpitutorial/","page":"MPI Tutorial","title":"MPI Tutorial","text":"Design: tuned is designed with the following goals according to Fagg et al. from the university of Tennessee:","category":"page"},{"location":"mpitutorial/","page":"MPI Tutorial","title":"MPI Tutorial","text":"Multiple collective implementations\nMultiple logical topologies\nWide range of fully tunable parameters\nEffficient default decisions\nAlternative user supplied compiled decision functions\nUser supplied selective decision parameter configuration files\nProvide a means to dynamically create/alter decision functions at runtime","category":"page"},{"location":"mpitutorial/","page":"MPI Tutorial","title":"MPI Tutorial","text":"For the DPHPC project, the most important feature that we need is the 5th item, as we can make changes to the code and tune for better performance without fully replacing the current default rules.","category":"page"},{"location":"mpitutorial/","page":"MPI Tutorial","title":"MPI Tutorial","text":"Performance:","category":"page"},{"location":"mpitutorial/","page":"MPI Tutorial","title":"MPI Tutorial","text":"(Source: Fagg et al)","category":"page"},{"location":"mpitutorial/","page":"MPI Tutorial","title":"MPI Tutorial","text":"","category":"page"},{"location":"mpitutorial/#MPI-Tutorial","page":"MPI Tutorial","title":"MPI Tutorial","text":"","category":"section"},{"location":"mpitutorial/","page":"MPI Tutorial","title":"MPI Tutorial","text":"Page documented some most important concepts about MPI (source)","category":"page"},{"location":"mpitutorial/#Introduction-and-MPI-installation","page":"MPI Tutorial","title":"Introduction and MPI installation","text":"","category":"section"},{"location":"mpitutorial/","page":"MPI Tutorial","title":"MPI Tutorial","text":"Point-to-point communication: communications involve one sender and receiver.","category":"page"},{"location":"mpitutorial/","page":"MPI Tutorial","title":"MPI Tutorial","text":"Collective communication: ","category":"page"},{"location":"mpitutorial/","page":"MPI Tutorial","title":"MPI Tutorial","text":"eg. when a manger process needs to broadcast information to all of its worker processes.","category":"page"},{"location":"mpitutorial/#Blocking-point-to-point-communication","page":"MPI Tutorial","title":"Blocking point-to-point communication","text":"","category":"section"},{"location":"mpitutorial/#Send-and-Receive","page":"MPI Tutorial","title":"Send and Receive","text":"","category":"section"},{"location":"mpitutorial/","page":"MPI Tutorial","title":"MPI Tutorial","text":"MPI_Send(\n    void* data,\n    int count,\n    MPI_Datatype datatype,\n    int destination,\n    int tag,\n    MPI_Comm communicator)","category":"page"},{"location":"mpitutorial/","page":"MPI Tutorial","title":"MPI Tutorial","text":"MPI_Recv(\n    void* data,\n    int count,\n    MPI_Datatype datatype,\n    int source,\n    int tag,\n    MPI_Comm communicator,\n    MPI_Status* status)","category":"page"},{"location":"mpitutorial/","page":"MPI Tutorial","title":"MPI Tutorial","text":"Process A decides a message needs to be sent to process B, process A packs up all of its necessary data into a buffer for process B. The buffer is called envelopes.","category":"page"},{"location":"mpitutorial/","page":"MPI Tutorial","title":"MPI Tutorial","text":"The communication device (often a network) is responsible for routing the message to the proper location.","category":"page"},{"location":"mpitutorial/","page":"MPI Tutorial","title":"MPI Tutorial","text":"Process B needs to acknowledge that it wants to receive A's data.","category":"page"},{"location":"mpitutorial/","page":"MPI Tutorial","title":"MPI Tutorial","text":"Tagged messages to be received by MPI_Recv() have priority over the messages with other tags => will be buffered","category":"page"},{"location":"mpitutorial/#Groups-and-communicators","page":"MPI Tutorial","title":"Groups and communicators","text":"","category":"section"},{"location":"mpitutorial/#Collective-communication","page":"MPI Tutorial","title":"Collective communication","text":"","category":"section"},{"location":"mpitutorial/","page":"MPI Tutorial","title":"MPI Tutorial","text":"We can classify the MPI collective operations using the following communication patterns:","category":"page"},{"location":"mpitutorial/","page":"MPI Tutorial","title":"MPI Tutorial","text":"1. One-to-many/Many-to-one: single producer or consumer (eg. broadcast, reduce, Scatter(v), Gather(v))","category":"page"},{"location":"mpitutorial/","page":"MPI Tutorial","title":"MPI Tutorial","text":"The data flow for this type of algorithm is unidirection. Virtual topologies can be used to determin the preceding and succeeding nodes in the algorithm.","category":"page"},{"location":"mpitutorial/","page":"MPI Tutorial","title":"MPI Tutorial","text":"i). receive data from preceding nodes(s)\nii). process data, if required\niii). send data to succeeding node(s)","category":"page"},{"location":"mpitutorial/","page":"MPI Tutorial","title":"MPI Tutorial","text":"2. Many-to-many: every participant is both producer and consumer (eg. barrier, alltoall, Allreduce, and Allgather(v))","category":"page"},{"location":"mpitutorial/","page":"MPI Tutorial","title":"MPI Tutorial","text":"(source: Pjesivac-Grbovic et al.)","category":"page"},{"location":"#Slim-Fly-MPI-Collective-Optimization","page":"Home","title":"Slim Fly MPI Collective Optimization","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The goal of the project is to optimize the MPI collective used on an existing cluster whose underlying network topology is based on the Slim Fly network topology. ","category":"page"},{"location":"","page":"Home","title":"Home","text":"Source: Slim Fly: A Cost Effective Low-Diameter Network Topology (Maciej Besta, Torsten Hoefler)","category":"page"},{"location":"#Getting-Started","page":"Home","title":"Getting Started","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The current MPI version we are using is OpenMPI of version 1.10.7.","category":"page"},{"location":"","page":"Home","title":"Home","text":"# by using mpiexec --version\n\nmpiexec (OpenRTE) 1.10.7","category":"page"},{"location":"","page":"Home","title":"Home","text":"# Activate the spack env in /home/dphpc/scratch/dphpc/wyou\nspacktivate .\n\n# Go into wyou/openmpi/ompi/mca/coll/tuned/ here you find the collectives\ncd openmpi/ompi/mca/coll/tuned\n\n# Modify the collective and spack install again (we should find a way to not reinstall everything every time because it takes forever)\n\n\n# In wyou/benchmarks/ there are some configs (mainly hostfiles) for testing. The main command to test a collective is this one (using osu microbenchmarks):\n\n$(which mpirun) -x PATH=$PATH -x LD_LIBRARY_PATH=$LD_LIBRARY_PATH -mca btl self,openib -mca btl_openib_allow_ib 1 -mca btl_openib_if_include mlx4_0 -mca btl_openib_ib_path_selection_strategy 0 -mca btl_openib_max_lmc 0 -mca btl_openib_enable_apm_over_lmc 0 -mca btl_openib_btls_per_lid 1 -mca pml bfo -mca btl_openib_ib_path_record_service_level 1 -mca coll ^tuned -mca coll_basic_priority 100000 -mca coll_basic_crossover 100 -np 8 --npernode 1 --hostfile ./8nodes osu_bcast -m 100000:5242880 -i 1","category":"page"},{"location":"#Host-file","page":"Home","title":"Host file","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The host file contains names of all of the computers on which the MPI job will execute. Either use the flag --hostfile ./host_file or setting the environment variable called MPI_HOSTS to specify.","category":"page"},{"location":"routing/#Routing","page":"Routing","title":"Routing","text":"","category":"section"}]
}
